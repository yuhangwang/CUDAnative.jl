<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Intrinsics · CUDAnative.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/highlightjs/default.css" rel="stylesheet" type="text/css"/><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>CUDAnative.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="../../man/usage.html">Usage</a></li><li><a class="toctext" href="../../man/troubleshooting.html">Troubleshooting</a></li><li><a class="toctext" href="../../man/performance.html">Performance</a></li></ul></li><li><span class="toctext">Library</span><ul><li><a class="toctext" href="../compilation.html">Compilation &amp; Execution</a></li><li><a class="toctext" href="../reflection.html">Reflection</a></li><li><a class="toctext" href="../profiling.html">Profiling</a></li><li><span class="toctext">Device Code</span><ul><li class="current"><a class="toctext" href="intrinsics.html">Intrinsics</a><ul class="internal"><li><a class="toctext" href="#Indexing-and-Dimensions-1">Indexing and Dimensions</a></li><li><a class="toctext" href="#Memory-Types-1">Memory Types</a></li><li><a class="toctext" href="#Synchronization-1">Synchronization</a></li><li><a class="toctext" href="#Warp-Vote-1">Warp Vote</a></li><li><a class="toctext" href="#Warp-Shuffle-1">Warp Shuffle</a></li><li><a class="toctext" href="#Formatted-Output-1">Formatted Output</a></li></ul></li><li><a class="toctext" href="array.html">Arrays</a></li><li><a class="toctext" href="libdevice.html">libdevice</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Library</li><li>Device Code</li><li><a href="intrinsics.html">Intrinsics</a></li></ul><a class="edit-page" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/docs/src/lib/device/intrinsics.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Intrinsics</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Intrinsics-1" href="#Intrinsics-1">Intrinsics</a></h1><p>This section lists the package&#39;s public functionality that corresponds to special CUDA functions to be used in device code. It is loosely organized according to the <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/#c-language-extensions">C language extensions</a> appendix from the CUDA C programming guide. For more information about certain intrinsics, refer to the aforementioned NVIDIA documentation.</p><h2><a class="nav-anchor" id="Indexing-and-Dimensions-1" href="#Indexing-and-Dimensions-1">Indexing and Dimensions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.gridDim" href="#CUDAnative.gridDim"><code>CUDAnative.gridDim</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">gridDim()::CuDim3</code></pre><p>Returns the dimensions of the grid.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/indexing.jl#L25-L29">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.blockIdx" href="#CUDAnative.blockIdx"><code>CUDAnative.blockIdx</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">blockIdx()::CuDim3</code></pre><p>Returns the block index within the grid.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/indexing.jl#L32-L36">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.blockDim" href="#CUDAnative.blockDim"><code>CUDAnative.blockDim</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">blockDim()::CuDim3</code></pre><p>Returns the dimensions of the block.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/indexing.jl#L39-L43">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.threadIdx" href="#CUDAnative.threadIdx"><code>CUDAnative.threadIdx</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">threadIdx()::CuDim3</code></pre><p>Returns the thread index within the block. </p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/indexing.jl#L46-L50">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.warpsize" href="#CUDAnative.warpsize"><code>CUDAnative.warpsize</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">warpsize()::UInt32</code></pre><p>Returns the warp size (in threads).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/indexing.jl#L53-L57">source</a><br/></section><h2><a class="nav-anchor" id="Memory-Types-1" href="#Memory-Types-1">Memory Types</a></h2><h3><a class="nav-anchor" id="Shared-Memory-1" href="#Shared-Memory-1">Shared Memory</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@cuStaticSharedMem" href="#CUDAnative.@cuStaticSharedMem"><code>CUDAnative.@cuStaticSharedMem</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">@cuStaticSharedMem(typ::Type, dims) -&gt; CuDeviceArray{typ}</code></pre><p>Get an array of type <code>typ</code> and dimensions <code>dims</code> (either an integer length or tuple shape) pointing to a statically-allocated piece of shared memory. The type should be statically inferable and the dimensions should be constant (without requiring constant propagation, see JuliaLang/julia#5560), or an error will be thrown and the generator function will be called dynamically.</p><p>Multiple statically-allocated shared memory arrays can be requested by calling this macro multiple times.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/memory_shared.jl#L30-L41">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@cuDynamicSharedMem" href="#CUDAnative.@cuDynamicSharedMem"><code>CUDAnative.@cuDynamicSharedMem</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">@cuDynamicSharedMem(typ::Type, dims, offset::Integer=0) -&gt; CuDeviceArray{typ}</code></pre><p>Get an array of type <code>typ</code> and dimensions <code>dims</code> (either an integer length or tuple shape) pointing to a dynamically-allocated piece of shared memory. The type should be statically inferable and the dimension and offset parameters should be constant (without requiring constant propagation, see JuliaLang/julia#5560), or an error will be thrown and the generator function will be called dynamically.</p><p>Dynamic shared memory also needs to be allocated beforehand, when calling the kernel.</p><p>Optionally, an offset parameter indicating how many bytes to add to the base shared memory pointer can be specified. This is useful when dealing with a heterogeneous buffer of dynamic shared memory; in the case of a homogeneous multi-part buffer it is preferred to use <code>view</code>.</p><p>Note that calling this macro multiple times does not result in different shared arrays; only a single dynamically-allocated shared memory array exists.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/memory_shared.jl#L84-L101">source</a><br/></section><h2><a class="nav-anchor" id="Synchronization-1" href="#Synchronization-1">Synchronization</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.sync_threads" href="#CUDAnative.sync_threads"><code>CUDAnative.sync_threads</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">sync_threads()</code></pre><p>Waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to <code>sync_threads()</code> are visible to all threads in the block.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/synchronization.jl#L5-L11">source</a><br/></section><h2><a class="nav-anchor" id="Warp-Vote-1" href="#Warp-Vote-1">Warp Vote</a></h2><p>The warp vote functions allow the threads of a given warp to perform a reduction-and-broadcast operation. These functions take as input a boolean predicate from each thread in the warp and evaluate it. The results of that evaluation are combined (reduced) across the active threads of the warp in one different ways, broadcasting a single return value to each participating thread.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.vote_all" href="#CUDAnative.vote_all"><code>CUDAnative.vote_all</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">vote_all(predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return non-zero if and only if <code>predicate</code> evaluates to non-zero for all of them.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/warp_vote.jl#L13-L18">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.vote_any" href="#CUDAnative.vote_any"><code>CUDAnative.vote_any</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">vote_any(predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return non-zero if and only if <code>predicate</code> evaluates to non-zero for any of them.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/warp_vote.jl#L34-L39">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.vote_ballot" href="#CUDAnative.vote_ballot"><code>CUDAnative.vote_ballot</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">vote_ballot(predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return an integer whose Nth bit is set if and only if <code>predicate</code> evaluates to non-zero for the Nth thread of the warp and the Nth thread is active.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/warp_vote.jl#L53-L59">source</a><br/></section><h2><a class="nav-anchor" id="Warp-Shuffle-1" href="#Warp-Shuffle-1">Warp Shuffle</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl" href="#CUDAnative.shfl"><code>CUDAnative.shfl</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">shfl_idx(val, src::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a directly indexed lane <code>src</code></p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/warp_shuffle.jl#L57-L61">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_up" href="#CUDAnative.shfl_up"><code>CUDAnative.shfl_up</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">shfl_up(val, src::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane with lower ID relative to caller.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/warp_shuffle.jl#L63-L67">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_down" href="#CUDAnative.shfl_down"><code>CUDAnative.shfl_down</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">shfl_down(val, src::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane with higher ID relative to caller.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/warp_shuffle.jl#L69-L73">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_xor" href="#CUDAnative.shfl_xor"><code>CUDAnative.shfl_xor</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">shfl_xor(val, src::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane based on bitwise XOR of own lane ID.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/warp_shuffle.jl#L75-L79">source</a><br/></section><h2><a class="nav-anchor" id="Formatted-Output-1" href="#Formatted-Output-1">Formatted Output</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@cuprintf" href="#CUDAnative.@cuprintf"><code>CUDAnative.@cuprintf</code></a> — <span class="docstring-category">Macro</span>.</div><div><p>Print a formatted string in device context on the host standard output:</p><pre><code class="language-none">@cuprintf(&quot;%Fmt&quot;, args...)</code></pre><p>Note that this is not a fully C-compliant <code>printf</code> implementation; see the CUDA documentation for supported options and inputs.</p><p>Also beware that it is an untyped, and unforgiving <code>printf</code> implementation. Type widths need to match, eg. printing a 64-bit Julia integer requires the <code>%ld</code> formatting string.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/8e19e34904c9a3c7a3c71d8834ccfa070a99d416/src/device/intrinsics/output.jl#L7-L17">source</a><br/></section><footer><hr/><a class="previous" href="../profiling.html"><span class="direction">Previous</span><span class="title">Profiling</span></a><a class="next" href="array.html"><span class="direction">Next</span><span class="title">Arrays</span></a></footer></article></body></html>
